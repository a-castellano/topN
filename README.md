# topN

[![pipeline status](https://git.windmaker.net/a-castellano/topN/badges/master/pipeline.svg)](https://git.windmaker.net/a-castellano/topN/commits/master)

## Statement

Write a program, topN, that given an arbitrarily large file and a number, N, containing individual numbers on each line (e.g. 200Gb file), will output the largest N numbers, highest first. Tell me about the run time/space complexity of it, and whether you think thereâ€™s room for improvement in your approach.

## Time/Space complexity

### Time complexity

Insert new elements has the following cost:
*  Insert new value => O(log N)
*  Look for mimum value inside heap => O(N)
*  Delete minimun value => O(log N)
*  Re-establish the heap Order => O(log N)

There are M elements stored in read file, so this solution has a time complexity of O(MN).

Time complexity could be O(MlogN) but not removing unnecessary elements from our heap will end in unacceptable memory consumption.

### Space complexity

First version of [Push](https://git.windmaker.net/a-castellano/topN/blob/ee95d764a6e804633ce705d0d174485a35cfd961/top/top.go#L26)  function was only adding new elements to the heap. The more elements file has the more memory our implementation would need, with large files this means unacceptable memory usage (my current Linux VM has only 1Gb RAM).

For this reason, when heap's size reaches N, current **Push** version deletes minimum value contained inside heap for keeping N elements inside of it.

## Future Improvements

I should look for a data structure or heap interface that allows me to find heap's minimun value in less complexity than O(N).

The following implementation spends around 45 seconds to find fist 70 elements inside 1.3Gb file. The execution of this program against 200Gb file will take more than 3 hours to finish. As memory use is enclosed and heap won't have more than N+1 elements large files can be split to parallelize our program execution and merge final heaps at the end.